Obscured-by-Clouds (OBC) - Documentation v0.91
==============================================

Thank you for using Logic and Space products!

For more information about Logic and Space you can visit: https://logicand.space


Project Overview
----------------
OBC is a modular ETL pipeline that extracts, transforms, and loads transaction data locally, with anonymisation, validation, logging, and terminal UI support.
The project is designed to prepare data for future cloud integration while maintaining privacy, reproducibility, and modularity.


ðŸ”§ Interactive Terminal Preview
--------------------------------
- Options:

   1. Extract and Transform Print
   10. Preview first 10 rows (both extracted & transformed)
   2. Extract and Load to CSV file
   3. Extract Transform and Load to CSV file
   4. Extract Transform and Load to Data Base
   5. Upload raw CSV file to Cloud (AWS) s3 Bucket (coming soon)
   6. Upload PII fields hashed CSV to Cloud (AWS) (coming soon)
   D for Documentation you can reach this documentation page inside the obc_app.

- Terminal output uses **Rich** for aligned tables and clear readability.


ðŸš€ Quick Start / How to Use
---------------------------
1. **Clone the repository**:
   $ git clone https://github.com/Eng-ADAL/Obscured-by-Clouds
   $ cd Obscured-by-Clouds

2. **Run setup script** to create Python virtual environment and install dependencies:
   - Linux/macOS:
     $ ./setup.sh
   - Windows PowerShell:
     > .\setup.ps1

3. **Prepare your raw data**:
   - Place your raw transaction file in:
     `data_raw/raw_data.txt`
   - Sample raw data:
     
     Customer: John Doe
     TransactionID: TX123456
     Item: Coffee
     Price: Â£12.99
     Date: 13/11/2025

   - This file will be read by the ETL pipeline for extraction.

4. **Run Dockerised app**:
   $ docker compose run --rm obc_app

5. **Preview data in the terminal (optional)**:
   - Select option `10` to display first 10 rows of extracted/transformed data.
   - The app will show:
     - Extracted raw data (hashed/anonymised)
     - Transformed 3NF structure (Transactions, Basket Items)
     - Abbreviated Transaction IDs & Customer Hashes
     - Date shortened (YYYY-MM-DD)


ðŸ“‚ File Structure
-----------------
Obscured-by-Clouds/
â”œâ”€ data_raw/             # Raw data input
â”‚  â””â”€ raw_data.txt
â”œâ”€â”€ data_ext             # Extracted data stored 
â”‚Â Â  â”œâ”€â”€ el_csv_data.csv
â”‚Â Â  â””â”€â”€ etl_csv_data.csv
â”œâ”€ source/
â”‚  â”œâ”€ app.py             # Main entry point
â”‚  â”œâ”€ extract.py         # Extract stage
â”‚  â”œâ”€ transform.py       # Transform stage
â”‚  â”œâ”€ load.py            # Load stage
â”‚  â”œâ”€ ui.py              # Interactive menu & preview
â”‚  â””â”€ utils.py           # Helper functions
â””â”€â”€ sql
â”‚   â””â”€â”€ create_tables.sql  # Sql tables
â”œâ”€ setup.sh              # Linux/macOS environment setup
â”œâ”€ setup.ps1             # Windows PowerShell setup
â”œâ”€ requirements.txt      # Python dependencies
â””â”€ README.md


ðŸ“Š ETL Process Overview
-----------------------
1. Extract
   - Reads `raw_data.txt`.
   - Hashes PII fields (Customer names).
   - Cleans missing/invalid values.

2. Transform
   - Converts data into 3NF structure:
     - Transactions
     - Basket Items
   - Abbreviates IDs & shortens dates.
   - Logs invalid rows for review.

3. Load
   - Writes processed data to:
     - `local_database/`
   - Ensures data integrity & reproducibility.


ðŸ“ Sample Extracted Data
------------------------

| CustomerHash | TransactionID | Item   | Price   | Date       |
| ------------ | ------------- | ------ | ------- | ---------- |
| a1b2c3d4e5   | TX123***      | Coffee |    3.99 | 2025-11-13 |
| f6g7h8i9j0   | TX124***      | Tea    |    1.50 | 2025-11-13 |



ðŸ’¡ Notes
--------
- Always ensure `raw_data.txt` exists before running the ETL.
- Option `10` in the app is useful for quick inspection of first N rows.
- Logs are written locally for debugging and reproducibility.
- `setup.sh` and `setup.ps1` automate environment setup; check Python 3.10+ is installed.


Modules
-------
1. extract.py
   - Purpose: Extract raw data from `data_raw/raw_data.txt` and anonymise PII.
   - Key functions:
     - extract_txt(filepath: str) -> list[dict]: Reads raw text, parses into structured dicts.
     - Anonymisation: Customer names, emails, etc., are hashed using salted hash.
   - Logs skipped or malformed rows for debugging.

2. transform.py
   - Purpose: Validate and transform extracted data into 3NF-ready structure.
   - Key functions:
     - decimal_price(row: dict) -> Decimal: Converts price to decimal, skips invalid entries.
     - transform_all(data: list[dict]) -> dict: Produces transformed transactions and basket items.
   - Features:
     - Shortened Transaction IDs and Customer Hashes for terminal display.
     - Skipped-row logging for invalid prices or missing fields.

3. load.py
   - Purpose: Load transformed data into local database.
   - Key functions:
     - load_transactions(transactions: list[dict]): Inserts transactions with GUIDs.
     - load_basket_items(basket_items: list[dict]): Inserts basket items with GUIDs linked to transactions.
   - Integrates with `dal.py` (Data Access Layer) for DB abstraction.

4. create_table.sql
   - Purpose: Provides abstraction layer for database access.
   - Functions:
     - insert(table: str, data: list[dict])
     - query(table: str, filters: dict = None)
   - Ensures GUID consistency and safe insertion.

5. ui.py
   - Purpose: Interactive terminal interface for ETL stats and filtered views.
   - Key functions:
     - extract_transform_to_terminal(limit=None, transform=True): Shows extraction and transformation stats.
     - Table displays via Rich library for pretty printing.
     - Supports filtering, limited preview, and statistics summary.

6. app.py (entry point)
   - Purpose: Orchestrates full ETL process.
   - Usage:
     - Runs extraction, transformation, loading, and optionally triggers UI stats.
     - CLI-friendly: can specify record limit for testing.


Data Flow
---------
raw_data.txt -> extract.py (anonymise) -> transform.py (validate & 3NF-ready) -> load.py (insert GUIDs) -> local_database

Logging
-------
- Skipped rows due to invalid prices or missing data.
- Transformation warnings for anomalies.
- Logs stored in `logs/` with timestamped filenames.

Design Principles
-----------------
- Privacy-first: Immediate PII hashing.
- Transparency: Local logging and stats.
- Modularity: Independent ETL components.
- Reproducibility: Deterministic transformations.
- Scalability-ready: Components can migrate to cloud orchestration.

Roadmap
-------
- [x] Local ETL pipeline with terminal UI
- [x] GUID-based 3NF-ready transformations
- [x] Skipped-row logging
- [ ] Unit testing for each module
- [ ] Cloud ETL orchestration
- [ ] CLI automation
- [ ] Extended documentation with diagrams



ðŸ’» Quick Cheat Sheet
-------------------
# Activate Python environment (Linux/macOS)
$ source obc_venv/bin/activate

# Activate Python environment (Windows PowerShell)
> .\obc_venv\Scripts\activate

# Run CLI entry point
$ python3 source/app.py

# Preview first 10 rows of extracted and transformed data
# Option 10 in the interactive menu

# Check Python dependencies (if you modify code or add packages)
$ pip install -r requirements.txt

# Run setup script again (Linux/macOS)
$ ./setup.sh

# Run setup script again (Windows PowerShell)
> .\setup.ps1

# Run dockerised app
$ docker compose run --rm obc_app

# Sample path for raw data file
data_raw/raw_data.txt

# Extracted and loaded csv data file
data_raw/raw_data.txt


ðŸ“ž Contact & Support
-----------------
For questions or contributions:
GitHub: https://github.com/Eng-ADAL/Obscured-by-Clouds
Website: https://logicand.space
